The question of how we interact with hyper aware semi-intelligent technology is thorny one. The old ways of using directed interface like keyboards or touch interfaces are inadequate. Not because they are insufficient to the task of controlling or directing action but because the paradigm is inefficient and unnecessary in this new world. Semi-intelligent machines are aware, they watch us as much as we watch them, and they observe and think. Just as you would not ask someone to do something only to then physically guide their hands and feet to the proper locations, just so you would not do for semi-intelligent machines. 

There is a new paradigm forming between humans and their hyper aware semi-intelligent machines. This new set of behaviors and expectations will at first be much like how we interface with our pets, then eventually once machines evolve past that level, we will interact with them like people. If the probabilities align, if machines ever evolve beyond human intelligence then there is no paradigm we can fall back upon and something new will ultimately come about. 

For now I’d imagine people would interact with the Wall through natural language, using the Google Speech API, translating natural spoken words into a command language which the Wall can understand. Coupled with posture and face tracking to resolve their emotive states, the Wall can respond to the gestures and intention of people at an intuitive level. No words need be spoken when the user wants to form a doorway or open a window. The Wall can change color and adapt to the moods and habits of the user without spoken commands. The Wall is as much an exploration of robotics, machine intelligence as it is of human to machine interaction, a prelude of what to come. 
